{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: flex-end; align-items: center;\">\n",
    "    <div style=\"width: 30%; text-align: right; margin-right: 20px;\">\n",
    "        <img src=\"https://www.juntadeandalucia.es/datosabiertos/portal/uploads/group/2022-09-06-135504.979247fps.png\" alt=\"Web Fundación Progreso y Salud\" style=\"width: 100%;\"/>\n",
    "    </div>\n",
    "    <div style=\"width: 60%; margin-right: 1em;\">\n",
    "        <p style=\"text-align: right; font-weight: bold; font-size: 2em; margin-top: 30px;\">\n",
    "            Scrape PubMed based on MeSH and Spanish text\n",
    "        </p>\n",
    "        <p style=\"text-align: right; color: #666\">\n",
    "            Proyect: MedicoderICD\n",
    "        </p>\n",
    "        <p style=\"text-align: right; color: #666\">\n",
    "            Juan Luis González Rodríguez\n",
    "        </p>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "from requests.exceptions import RequestException\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_none_in_tuple(t):\n",
    "    \"\"\"\n",
    "    Check if there is None or empty string in the tuple\n",
    "    :param t: tuple\n",
    "    :return: None if there is None or empty string in the tuple, otherwise return the tuple\n",
    "    \"\"\"\n",
    "    if t is None:\n",
    "        return None\n",
    "    if t == ('',) or t == ('', '') or t == ([''],) or t == ('', ['']):\n",
    "        return None\n",
    "    for element in t:\n",
    "        if element is None or element == '':\n",
    "            return None\n",
    "        elif type(element) == list:\n",
    "            if not element or '' in element:\n",
    "                return None\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_english(text):\n",
    "    \"\"\"Removes English phrases from the given text, preserving sentence structure.\"\"\"\n",
    "\n",
    "    phrases = text.split('.')  # Split into phrases, maintaining original sentence structure\n",
    "\n",
    "    spanish_phrases = []\n",
    "    for phrase in phrases:\n",
    "        lang = detect(phrase)  # Detect language using `langdetect`\n",
    "\n",
    "        if lang == \"en\":  # Exclude English phrases\n",
    "            break\n",
    "        spanish_phrases.append(phrase)\n",
    "\n",
    "    return '.'.join(spanish_phrases)  # Rejoin remaining phrases preserving structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_quotes_breakline_spaces(string):\n",
    "    string = string.replace(\"\\\"\", \"\\'\")\n",
    "    string = string.replace(\"\\n\", \"\")\n",
    "    string = ' '.join(string.split())\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoArticlesFoundError(Exception):\n",
    "    \"\"\"Excepción personalizada para indicar que no se encontraron artículos asociados al término Mesh en PubMed.\"\"\"\n",
    "\n",
    "    def __init__(self, message=\"No se encontraron artículos asociados al término Mesh en PubMed.\"):\n",
    "        self.message = message\n",
    "        super().__init__(self.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Publication:\n",
    "    def __init__(self, tuple_info):\n",
    "        self.pmid = tuple_info[0]\n",
    "        self.title = tuple_info[1]\n",
    "        self.abstract = tuple_info[2]\n",
    "        self.mesh_list = tuple_info[3]\n",
    "\n",
    "    def mesh_in_pub(self, mesh_desired):\n",
    "        return mesh_desired in self.mesh_list\n",
    "\n",
    "    def is_mesh_in_pub_list(self, mesh_reference):\n",
    "        return mesh_reference in self.mesh_list\n",
    "\n",
    "    def pub_to_csv(self, mesh_major_topic):\n",
    "        line = f'\\n\"{self.pmid}\",\"{mesh_major_topic}\", \"{self.title}\", \"{self.abstract}\", \"('\n",
    "        for mesh in self.mesh_list:\n",
    "            line += f\"{mesh}, \"\n",
    "        line = line[:-2]\n",
    "        line += ')\"'\n",
    "        return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElsevierScrapper:\n",
    "    \n",
    "    def __init__(self, doi, keypath):\n",
    "        self.doi = doi\n",
    "        self.apikey = self.get_api_key(keypath)\n",
    "    \n",
    "    def get_api_key(self, key_path):\n",
    "        with open(key_path, \"r\") as file:\n",
    "            return file.read().strip()\n",
    "        \n",
    "    def scrape_publication(self):\n",
    "        try:\n",
    "            url = f\"https://api.elsevier.com/content/article/doi/{self.doi}?apiKey={self.apikey}\"\n",
    "            response = requests.get(url)\n",
    "\n",
    "            if response.status_code != 200:\n",
    "                raise RequestException(f\"Error en la solicitud: {response.status_code}. La publicación no existe en Elsevier y por tanto no se puede obtener la información.\")\n",
    "            else:\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "                title_es = soup.find(\"dc:title\")\n",
    "                abstract_es = soup.find(\"dc:description\")\n",
    "                \n",
    "            return title_es.text, abstract_es.text\n",
    "\n",
    "        except RequestException as e:\n",
    "            print(f'Error en la solicitud: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PublicationScraper:\n",
    "    def __init__(self, publication_url):\n",
    "        self.url = publication_url\n",
    "\n",
    "    def scrape_spanish_text(self, url):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            title_es = soup.find(\"span\", class_=\"title-text\").text.strip()\n",
    "            title_es = clean_quotes_breakline_spaces(title_es)\n",
    "\n",
    "            abstract_elements = soup.find_all(class_=\"abstract_author\")\n",
    "            abstract_es = \"\"\n",
    "            for abstract_element in abstract_elements:\n",
    "                abstract_es += abstract_element.get_text()\n",
    "\n",
    "            return title_es, abstract_es\n",
    "        \n",
    "        except RequestException as e:\n",
    "            print(f'Error en la solicitud: {e}')\n",
    "        except Exception as e:\n",
    "            print(f'Ocurrió un error inesperado: {e}')\n",
    "        \n",
    "    def scrape_publication(self):\n",
    "        try:\n",
    "            response = requests.get(self.url)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            pmid = soup.find(\"span\", class_=\"identifier pubmed\").find(\"strong\").text\n",
    "            try:\n",
    "                doi = soup.find(\"span\", class_=\"identifier doi\").find(\"a\").text.strip()\n",
    "                doi = clean_quotes_breakline_spaces(doi)\n",
    "            except AttributeError:\n",
    "\n",
    "                raise RequestException(f\"No se encontró el enlace DOI para la publicacion con PMID: {pmid}\")\n",
    "\n",
    "            elsevier_scraper = ElsevierScrapper(doi, \"../data/apikey_elseiver.txt\")\n",
    "            title_es, abstract_es = elsevier_scraper.scrape_publication()\n",
    "\n",
    "            # Clean title and abstract\n",
    "            title_es = clean_quotes_breakline_spaces(title_es.strip())\n",
    "            abstract_es = remove_english(clean_quotes_breakline_spaces(abstract_es.strip()))\n",
    "\n",
    "                \n",
    "\n",
    "            mesh_set = {re.sub(\"\\s/\\s.*|\\*\", \"\", button.text.strip()) for button in\n",
    "                        soup.select(\"div.mesh-terms ul.keywords-list button\")}\n",
    "            mesh_list = list(mesh_set)\n",
    "\n",
    "            return pmid, title_es, abstract_es, mesh_list\n",
    "\n",
    "        except RequestException as e:\n",
    "            print(f'Error en la solicitud: {e}')\n",
    "        except Exception as e:\n",
    "            print(f'Ocurrió un error inesperado: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PubmedScraper:\n",
    "    def __init__(self, mesh_text):\n",
    "        self.base_url = \"https://pubmed.ncbi.nlm.nih.gov/\"\n",
    "        self.mesh_text = mesh_text\n",
    "        self.current_page = 1\n",
    "        self.max_pages = 10\n",
    "\n",
    "    def scrape_articles_current_page(self):\n",
    "        try:\n",
    "            urls = []\n",
    "\n",
    "            if self.current_page <= self.max_pages:\n",
    "                query = f\"{self.base_url}?term=({self.mesh_text}%5BMeSH%20Major%20Topic%5D)%20AND%20(Spanish%5BLanguage%5D)&filter=simsearch1.fha&page={self.current_page}\"\n",
    "                self.current_page += 1\n",
    "\n",
    "                response = requests.get(query)\n",
    "                response.raise_for_status()\n",
    "\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                articles = soup.find_all(\"article\", class_=\"full-docsum\")\n",
    "\n",
    "                if not articles:\n",
    "                    raise NoArticlesFoundError(f\"No se encontraron artículos asociados al término \"\n",
    "                                               f\"Mesh \\\"{self.mesh_text}\\\" en PubMed.\")\n",
    "\n",
    "                urls.extend(\n",
    "                    [f\"{self.base_url}{article.find('a', class_='docsum-title')['data-ga-label']}/\" for article in\n",
    "                     articles])\n",
    "\n",
    "                return urls\n",
    "\n",
    "        except RequestException as e:\n",
    "            print(f'Error en la solicitud: {e}')\n",
    "            return []\n",
    "        except Exception as e:\n",
    "            print(f'Ocurrió un error inesperado: {e}')\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_and_get_mesh(json_path):\n",
    "    \"\"\"\n",
    "    Gives the path of a json file, this function\n",
    "    extract the mesh name from the file and return it.\n",
    "\n",
    "    Args:\n",
    "        json_path (str): The path of the json file.\n",
    "\n",
    "    Returns:\n",
    "        str: The mesh name.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(json_path, 'r') as file:\n",
    "            # Load JSON data from the file\n",
    "            mesh_data = json.load(file)\n",
    "\n",
    "        # Access the \"DescriptorName\" key\n",
    "        return mesh_data[\"DescriptorName\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing the file {json_path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing MeSH terms: 100%|██████████| 5155/5155 [148:00:34<00:00, 103.36s/MeSH code]\n"
     ]
    }
   ],
   "source": [
    "# Read all existing mesh\n",
    "json_paths = os.path.join(\"..\", \"data\", \"meshs_json\")\n",
    "mesh_files = [os.path.join(\"..\", \"data\", \"meshs_json\", mesh) for mesh in os.listdir(json_paths) if mesh.endswith(\".json\")]\n",
    "all_mesh_list = []\n",
    "n_publications_per_mesh = 50\n",
    "\n",
    "# Get all the mesh names from the json files\n",
    "for mesh_path in tqdm(mesh_files, desc=\"Processing Files\", unit=\"file\"):\n",
    "    all_mesh_list.append(open_and_get_mesh(mesh_path))\n",
    "\n",
    "# Create the csv file and scrape the data\n",
    "csv_path = os.path.join(\"..\", \"data\", \"projects_pubmed.csv\")\n",
    "with open(csv_path, \"w\", encoding='utf-8') as csv_file:\n",
    "    csv_file.write(\"PMID, MESH_MAJOR_TOPIC, TITLE, ABSTRACT, MESH_TUPLE\")\n",
    "\n",
    "    for mesh in tqdm(all_mesh_list, desc=\"Processing MeSH terms\", unit=\"MeSH code\"):\n",
    "        print(f\"Processing MeSH term: {mesh}\")\n",
    "        list_publications = []\n",
    "\n",
    "        # Instancia del buscador\n",
    "        pubmed_engine = PubmedScraper(mesh)\n",
    "\n",
    "        while len(list_publications) < n_publications_per_mesh:\n",
    "            list_urls = pubmed_engine.scrape_articles_current_page()\n",
    "            if list_urls is None:\n",
    "                break\n",
    "\n",
    "            for url in list_urls:\n",
    "                # Parse each url into a tuple\n",
    "                tuple_pub = PublicationScraper(url).scrape_publication()\n",
    "\n",
    "                # Check if any element in the tuple is empty\n",
    "                tuple_pub = check_none_in_tuple(tuple_pub)\n",
    "\n",
    "                # If tuple as no info skip to next url\n",
    "                if tuple_pub is None:\n",
    "                    continue\n",
    "\n",
    "                # Create a publication obj\n",
    "                publication = Publication(tuple_pub)\n",
    "                if publication.is_mesh_in_pub_list(mesh):\n",
    "                    list_publications.append(publication)\n",
    "                    if len(list_publications) == n_publications_per_mesh:\n",
    "                        break\n",
    "\n",
    "        print(f\"Se han encontrado {len(list_publications)} publicaciones para el MeSH {mesh}\")\n",
    "        time.sleep(1)\n",
    "        # Resfres the output flush\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        for pub in list_publications:\n",
    "            csv_file.write(pub.pub_to_csv(mesh))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
